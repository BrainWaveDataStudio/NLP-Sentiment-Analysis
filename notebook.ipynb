{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer,\n",
    "    TfidfTransformer,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertForSequenceClassification, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"datasets/imdb\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "dataset = load_dataset(\"imdb\", cache_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy way to split a dataset into train and validation sets\n",
    "\n",
    "# train_df = dataset[\"train\"].to_pandas()\n",
    "# train_df, valid_df = train_test_split(train_df, test_size=0.2, random_state=0)\n",
    "\n",
    "# A more careful way to split a dataset into train and validation sets considering the label distribution\n",
    "\n",
    "full_df = dataset[\"train\"].to_pandas()\n",
    "np.random.seed(0)\n",
    "positive_df = full_df[full_df[\"label\"] == 1].sample(frac=1) # shuffle\n",
    "negative_df = full_df[full_df[\"label\"] == 0].sample(frac=1) # shuffle\n",
    "train_df = pd.concat([positive_df[:int(len(positive_df) * 0.8)], negative_df[:int(len(negative_df) * 0.8)]])\n",
    "valid_df = pd.concat([positive_df[int(len(positive_df) * 0.8):], negative_df[int(len(negative_df) * 0.8):]])\n",
    "\n",
    "test_df = dataset[\"test\"].to_pandas()\n",
    "\n",
    "print(\"Training set size:\", len(train_df))\n",
    "print(\"Validation set size:\", len(valid_df))\n",
    "print(\"Test set size:\", len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "use_lemma = True\n",
    "use_lower = True\n",
    "remove_punct = True\n",
    "\n",
    "\n",
    "def process(text, remove_punct=True, use_lemma=True, use_lower=True):\n",
    "    words, processed_words = [], []\n",
    "    for word in nlp(text):\n",
    "        words.append(word.text)\n",
    "        if word.is_space or (remove_punct and word.is_punct):\n",
    "            continue\n",
    "        processed_words.append(word.lemma_ if use_lemma else word.text)\n",
    "\n",
    "    if use_lower:\n",
    "        processed_words = [word.lower() for word in words]\n",
    "    \n",
    "    return \" \".join(words), \" \".join(processed_words)\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas() \n",
    "train_df[\"processed_text\"] = train_df[\"text\"].progress_apply(process)\n",
    "valid_df[\"processed_text\"] = valid_df[\"text\"].progress_apply(process)\n",
    "test_df[\"processed_text\"] = test_df[\"text\"].progress_apply(process)\n",
    "train_df[\"processed_text_length\"] = train_df[\"processed_text\"].apply(lambda x: len(x.split()))\n",
    "valid_df[\"processed_text_length\"] = valid_df[\"processed_text\"].apply(lambda x: len(x.split()))\n",
    "test_df[\"processed_text_length\"] = test_df[\"processed_text\"].apply(lambda x: len(x.split()))\n",
    "import tqdm.notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: you'll likely have to run the multiprocessing code as a separate script file\n",
    "\n",
    "remove_punct = True\n",
    "use_lemma = True\n",
    "use_lower = True\n",
    "\n",
    "\n",
    "def worker_initializer():\n",
    "    global nlp\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    \n",
    "def process(text, remove_punct=True, use_lemma=True, use_lower=True):\n",
    "    words, processed_words = [], []\n",
    "    for word in nlp(text):\n",
    "        words.append(word.text)\n",
    "        if word.is_space or (remove_punct and word.is_punct):\n",
    "            continue\n",
    "        processed_words.append(word.lemma_ if use_lemma else word.text)\n",
    "\n",
    "    if use_lower:\n",
    "        processed_words = [word.lower() for word in words]\n",
    "    \n",
    "    return \" \".join(words), \" \".join(processed_words)\n",
    "\n",
    "process_func = partial(process, remove_punct=remove_punct, use_lemma=use_lemma, use_lower=use_lower) \n",
    "\n",
    "with Pool(\n",
    "    processes=8,\n",
    "    initializer=worker_initializer,\n",
    ") as pool:\n",
    "    train_df[\"processed_text\"] = list(tqdm.tqdm(pool.imap(process, train_df[\"text\"], chunksize=1), total=len(train_df)))  \n",
    "    valid_df[\"processed_text\"] = list(tqdm.tqdm(pool.imap(process, valid_df[\"text\"], chunksize=1), total=len(valid_df)))\n",
    "    test_df[\"processed_text\"] = list(tqdm.tqdm(pool.imap(process, test_df[\"text\"], chunksize=1), total=len(test_df)))\n",
    "    \n",
    "train_df[\"processed_text_length\"] = train_df[\"processed_text\"].apply(lambda x: len(x.split()))\n",
    "valid_df[\"processed_text_length\"] = valid_df[\"processed_text\"].apply(lambda x: len(x.split()))\n",
    "test_df[\"processed_text_length\"] = test_df[\"processed_text\"].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"datasets/imdb/train.csv\", index=False)\n",
    "valid_df.to_csv(\"datasets/imdb/valid.csv\", index=False)\n",
    "test_df.to_csv(\"datasets/imdb/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"datasets/imdb/train.csv\")\n",
    "valid_df = pd.read_csv(\"datasets/imdb/valid.csv\")\n",
    "test_df = pd.read_csv(\"datasets/imdb/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"processed_text_length\"] = train_df[\"processed_text\"].apply(lambda x: len(x.split()))\n",
    "valid_df[\"processed_text_length\"] = valid_df[\"processed_text\"].apply(lambda x: len(x.split()))\n",
    "test_df[\"processed_text_length\"] = test_df[\"processed_text\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "sns.set_theme()\n",
    "sns.histplot(train_df[train_df[\"label\"] == 1][\"processed_text_length\"], label=\"train-positive\", ax=ax, stat=\"density\", kde=True, bins=20)\n",
    "sns.histplot(train_df[train_df[\"label\"] == 0][\"processed_text_length\"], label=\"train-negative\", ax=ax, stat=\"density\", kde=True, bins=20)\n",
    "sns.histplot(valid_df[valid_df[\"label\"] == 1][\"processed_text_length\"], label=\"valid-positive\", ax=ax, stat=\"density\", kde=True, bins=20)\n",
    "sns.histplot(valid_df[valid_df[\"label\"] == 0][\"processed_text_length\"], label=\"valid-negative\", ax=ax, stat=\"density\", kde=True, bins=20)\n",
    "fig.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vader Setiment Intesity Analyzer\n",
    "# Valid acc: 0.6914, Test acc: 0.6958\n",
    "import nltk\n",
    "nltk.download(\"vader_lexicon\")\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "preds = []\n",
    "for sentence in tqdm.tqdm(valid_df[\"processed_text\"]):\n",
    "    result = analyzer.polarity_scores(sentence)\n",
    "    preds.append(1 if result[\"pos\"] >= result[\"neg\"] else 0)\n",
    "preds = np.array(preds)\n",
    "\n",
    "print(\"Valid accuracy:\", (preds == valid_df[\"label\"]).mean())\n",
    "\n",
    "preds = []\n",
    "for sentence in tqdm.tqdm(test_df[\"processed_text\"]):\n",
    "    result = analyzer.polarity_scores(sentence)\n",
    "    preds.append(1 if result[\"pos\"] >= result[\"neg\"] else 0)\n",
    "preds = np.array(preds)\n",
    "\n",
    "print(\"Test accuracy:\", (preds == test_df[\"label\"]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression based on TruncatedSVD of TF-IDF of the processed texts\n",
    "# Valid acc: 0.8698, Test acc: 0.8606\n",
    "np.random.seed(0)\n",
    "\n",
    "model = Pipeline(\n",
    "    [\n",
    "        (\"vectorizer\", CountVectorizer(token_pattern=\"\\S+\", min_df=50, max_df=0.8, stop_words='english')),\n",
    "        (\"tfidf\", TfidfTransformer()),\n",
    "        (\"svd\", TruncatedSVD(n_components=2048)),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"classifier\", LogisticRegression(C=0.001))\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.fit(train_df[\"processed_text\"], train_df[\"label\"])\n",
    "\n",
    "print(\"Train accuracy:\", (model.predict(train_df[\"processed_text\"]) == train_df[\"label\"]).mean())\n",
    "print(\"Valid accuracy:\", (model.predict(valid_df[\"processed_text\"]) == valid_df[\"label\"]).mean())\n",
    "\n",
    "print(\"Test accuracy:\", (model.predict(test_df[\"processed_text\"]) == test_df[\"label\"]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the GloVe embeddings matrix for NN model (optional); you'll have to download the GloVe embeddings from https://nlp.stanford.edu/projects/glove/\n",
    "# Recommended reading: https://www.ruder.io/word-embeddings-1/\n",
    "# Thought experiment: why would we like to NOT use the pre-trained word embeddings? What are some of the advantages of training our own embeddings?\n",
    "\n",
    "word_counter = Counter(\n",
    "    [word for text in train_df[\"processed_text\"] for word in text.split(' ')] +\n",
    "    [word for text in valid_df[\"processed_text\"] for word in text.split(' ')]\n",
    ")\n",
    "\n",
    "word2idx, idx2word = {\"<UNK>\": 0, \"<PAD>\": 1}, [\"<UNK>\", \"<PAD>\"]\n",
    "\n",
    "covered = 0\n",
    "for word, count in word_counter.most_common()[:19998]:\n",
    "    word2idx[word] = len(idx2word)\n",
    "    idx2word.append(word)\n",
    "    covered += count\n",
    "print(f\"Word coverage: {covered / sum(word_counter.values()) * 100:.4f}%\")\n",
    "\n",
    "embeddings_matrix = np.random.uniform(-1, 1, (len(idx2word), 300)).astype(np.float32)\n",
    "embeddings_matrix[1, :] = 0\n",
    "with open(\"../datasets/glove/glove.6B.300d.txt\") as file:\n",
    "    for line in file:\n",
    "        chunks = line.split(' ')\n",
    "        word, embedding = chunks[0], np.array(chunks[1:], dtype=np.float32)\n",
    "        if word in word2idx:\n",
    "            embeddings_matrix[word2idx[word]] = embedding\n",
    "            \n",
    "embeddings = nn.Embedding.from_pretrained(torch.from_numpy(embeddings_matrix), padding_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, embeddings=None, vocab_size=None, embedding_dim=None):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        if embeddings is not None:\n",
    "            self._embeddings = embeddings\n",
    "        else:\n",
    "            assert vocab_size is not None and embedding_dim is not None\n",
    "            self._embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self._cnn_layers = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=self._embeddings.embedding_dim, out_channels=100, kernel_size=3, padding=\"same\"),\n",
    "            nn.Conv1d(in_channels=self._embeddings.embedding_dim, out_channels=100, kernel_size=4, padding=\"same\"),\n",
    "            nn.Conv1d(in_channels=self._embeddings.embedding_dim, out_channels=100, kernel_size=5, padding=\"same\"),\n",
    "        ])\n",
    "        self._avg_pooling = nn.AdaptiveAvgPool1d(1)\n",
    "        self._max_pooling = nn.AdaptiveMaxPool1d(1)\n",
    "        self._dropout = nn.Dropout(0.5)\n",
    "        self._linear = nn.Linear(600, 1)\n",
    "       \n",
    "    def forward(self, x: torch.LongTensor) -> torch.Tensor:\n",
    "        e = self._embeddings(x).permute(0, 2, 1)\n",
    "        c = torch.cat([cnn(e) for cnn in self._cnn_layers], dim=1)\n",
    "        c = self._dropout(F.relu(c))\n",
    "        p = torch.cat([self._avg_pooling(c), self._max_pooling(c)], dim=1).squeeze(2)\n",
    "        return torch.sigmoid(self._linear(p).squeeze())\n",
    "\n",
    "\n",
    "class CNNLSTMClassifier(nn.Module):\n",
    "    def __init__(self, embeddings=None, vocab_size=None, embedding_dim=None):\n",
    "        super(CNNLSTMClassifier, self).__init__()\n",
    "        if embeddings is not None:\n",
    "            self._embeddings = embeddings\n",
    "        else:\n",
    "            assert vocab_size is not None and embedding_dim is not None\n",
    "            self._embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self._cnn_layers = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=self._embeddings.embedding_dim, out_channels=100, kernel_size=3, padding=\"same\"),\n",
    "            nn.Conv1d(in_channels=self._embeddings.embedding_dim, out_channels=100, kernel_size=4, padding=\"same\"),\n",
    "            nn.Conv1d(in_channels=self._embeddings.embedding_dim, out_channels=100, kernel_size=5, padding=\"same\"),\n",
    "        ])\n",
    "        self._lstm = nn.LSTM(batch_first=True, input_size=600, hidden_size=300, bidirectional=True)\n",
    "        self._avg_pool = nn.AvgPool1d(kernel_size=2)\n",
    "        self._max_pool = nn.MaxPool1d(kernel_size=2)\n",
    "        self._cnn_dropout = nn.Dropout(0.4)\n",
    "        self._rnn_dropout = nn.Dropout(0.1)\n",
    "        self._linear = nn.Linear(600, 1)\n",
    "       \n",
    "    def forward(self, x: torch.LongTensor, l: torch.LongTensor) -> torch.Tensor:\n",
    "        e = self._embeddings(x).permute(0, 2, 1)\n",
    "        c = F.relu(torch.cat([cnn(e) for cnn in self._cnn_layers], dim=1))\n",
    "        c = self._cnn_dropout(c)\n",
    "        c = torch.cat([self._avg_pool(c), self._max_pool(c)], dim=1).permute(0, 2, 1)\n",
    "        c = pack_padded_sequence(c, l // 2, batch_first=True, enforce_sorted=False)\n",
    "        _, (h, _) = self._lstm(c)\n",
    "        return torch.sigmoid(self._linear(h.permute(1, 0, 2).reshape(x.shape[0], -1)).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, word2idx, texts, labels):\n",
    "        self._word2idx = word2idx\n",
    "        self._texts = texts\n",
    "        self._labels = labels\n",
    "        assert len(self._texts) == len(self._labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text, label = self._texts[index], self._labels[index]\n",
    "        tokens = torch.tensor(\n",
    "            [self._word2idx.get(word, 0) for word in text.split(' ')],\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        return tokens, torch.tensor(label, dtype=torch.float)\n",
    "    \n",
    "    \n",
    "def build_collate_fn(max_length=512):\n",
    "    def collate_fn(batch):\n",
    "        batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "        texts, labels = zip(*batch)\n",
    "        texts = [text[:max_length] for text in texts]\n",
    "        lengths = torch.LongTensor([len(text) for text in texts])\n",
    "        texts = pad_sequence(texts, batch_first=True, padding_value=1)\n",
    "        labels = torch.tensor(labels, dtype=torch.float)\n",
    "        return texts, labels, lengths\n",
    "    \n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IMDBDataset(word2idx, texts=train_df['processed_text'], labels=train_df['label'])\n",
    "valid_dataset = IMDBDataset(word2idx, texts=valid_df['processed_text'], labels=valid_df['label'])\n",
    "test_dataset = IMDBDataset(word2idx, texts=test_df['processed_text'], labels=test_df['label'])\n",
    "collate_fn = build_collate_fn(max_length=1024)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, num_epochs, train_dataloader, valid_dataloader):\n",
    "        self._num_epochs = num_epochs\n",
    "        self._train_dataloader = train_dataloader\n",
    "        self._valid_dataloader = valid_dataloader\n",
    "    \n",
    "    def _run_batch(self, batch):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def _inference(self, batch):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def _load_model(self, path=None):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def _save_model(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def train_epoch(self):\n",
    "        epoch_loss, count = 0.0, 0\n",
    "        epoch_bar = tqdm.tqdm(self._train_dataloader, leave=False)\n",
    "        \n",
    "        for batch in epoch_bar:\n",
    "            preds, batch_loss = self._run_batch(batch)\n",
    "            epoch_loss = epoch_loss * count + batch_loss * preds.shape[0] \n",
    "            count += preds.shape[0]\n",
    "            epoch_loss /= count\n",
    "            epoch_bar.set_description(f\"loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        return epoch_loss\n",
    "    \n",
    "    def evaluate(self, dataloader=None):\n",
    "        dataloader = self._valid_dataloader if dataloader is None else dataloader\n",
    "\n",
    "        eval_loss, eval_hit, count = 0.0, 0, 0 \n",
    "        for batch in tqdm.tqdm(dataloader, leave=False):\n",
    "            preds, batch_loss = self._inference(batch)\n",
    "            eval_loss = eval_loss * count + batch_loss * preds.shape[0]\n",
    "            count += preds.shape[0]\n",
    "            eval_loss /= count\n",
    "            eval_hit += (preds.round() == batch[1]).sum().item()\n",
    "            \n",
    "        return eval_loss, eval_hit / count\n",
    "           \n",
    "    def train(self):\n",
    "        best_acc = 0.0\n",
    "\n",
    "        train_bar = tqdm.trange(self._num_epochs)\n",
    "        for i in train_bar:            \n",
    "            epoch_loss = self.train_epoch()\n",
    "            train_bar.write(f\"train loss for epoch {i}: {epoch_loss:.4f}\")\n",
    "            eval_loss, eval_acc = self.evaluate()\n",
    "            train_bar.write(f\"valid loss for epoch {i}: {eval_loss:.4f}\")\n",
    "            train_bar.write(f\"valid acc for epoch {i}: {eval_acc:.4f}\") \n",
    "            \n",
    "            if eval_acc > best_acc:\n",
    "                best_acc = eval_acc\n",
    "                self._save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNTrainer(Trainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        network,\n",
    "        optimizer,\n",
    "        device,\n",
    "        num_epochs,\n",
    "        train_dataloader,\n",
    "        valid_dataloader,\n",
    "    ):\n",
    "        super().__init__(num_epochs, train_dataloader, valid_dataloader)\n",
    "        self._network = network\n",
    "        self._optimizer = optimizer\n",
    "        self._device = device\n",
    "        \n",
    "    def _run_batch(self, batch):\n",
    "        self._optimizer.zero_grad()\n",
    "        preds = self._network(batch[0].to(device=self._device))\n",
    "        loss = F.binary_cross_entropy(preds, batch[1].to(device=self._device))\n",
    "        loss.backward()\n",
    "        self._optimizer.step()\n",
    "        \n",
    "        return preds.cpu(), loss.item()\n",
    "    \n",
    "    def _inference(self, batch):\n",
    "        with torch.no_grad():\n",
    "            preds = self._network(batch[0].to(device=self._device))\n",
    "            loss = F.binary_cross_entropy(preds, batch[1].to(device=self._device))\n",
    "            preds = preds.round()\n",
    "            \n",
    "        return preds.cpu(), loss.item()\n",
    "    \n",
    "    def _load_model(self, path=None):\n",
    "        path = \"cnn_model.ckpt\" if path is None else path\n",
    "        self._network.load_state_dict(torch.load(path))\n",
    "    \n",
    "    def _save_model(self):\n",
    "        torch.save(self._network.state_dict(), \"cnn_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLSTMTrainer(Trainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        network,\n",
    "        optimizer,\n",
    "        device,\n",
    "        num_epochs,\n",
    "        train_dataloader,\n",
    "        valid_dataloader,\n",
    "    ):\n",
    "        super().__init__(num_epochs, train_dataloader, valid_dataloader)\n",
    "        self._network = network\n",
    "        self._optimizer = optimizer\n",
    "        self._device = device\n",
    "        \n",
    "    def _run_batch(self, batch):\n",
    "        self._optimizer.zero_grad()\n",
    "        preds = self._network(batch[0].to(device=self._device), batch[2])\n",
    "        loss = F.binary_cross_entropy(preds, batch[1].to(device=self._device))\n",
    "        loss.backward()\n",
    "        self._optimizer.step()\n",
    "        \n",
    "        return preds.cpu(), loss.item()\n",
    "    \n",
    "    def _inference(self, batch):\n",
    "        with torch.no_grad():\n",
    "            preds = self._network(batch[0].to(device=self._device), batch[2])\n",
    "            loss = F.binary_cross_entropy(preds, batch[1].to(device=self._device))\n",
    "            preds = preds.round()\n",
    "            \n",
    "        return preds.cpu(), loss.item()\n",
    "\n",
    "    def _load_model(self, path=None):\n",
    "        path = \"cnn_lstm_model.ckpt\" if path is None else path\n",
    "        self._network.load_state_dict(torch.load(path))\n",
    "    \n",
    "    def _save_model(self):\n",
    "        torch.save(self._network.state_dict(), \"cnn_lstm_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid acc: 0.8844, Test acc: 0.8789\n",
    "\n",
    "device = \"cpu\"\n",
    "# network = CNNClassifier(embeddings=embeddings).to(device=device)\n",
    "network = CNNClassifier(len(idx2word), 300).to(device=device)\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.001)\n",
    "trainer = CNNTrainer(network, optimizer, device, 5, train_dataloader, valid_dataloader)\n",
    "trainer.train()\n",
    "\n",
    "trainer._load_model()\n",
    "trainer.evaluate(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid acc: 0.8902, Test acc: 0.8720\n",
    "\n",
    "device = \"cpu\"\n",
    "# network = CNNLSTMClassifier(embeddings=embeddings).to(device=device)\n",
    "network = CNNLSTMClassifier(len(idx2word), 300).to(device=device)\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.001)\n",
    "trainer = CNNLSTMTrainer(network, optimizer, device, 5, train_dataloader, valid_dataloader)\n",
    "trainer.train()\n",
    "\n",
    "trainer._load_model()\n",
    "trainer.evaluate(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBBERTDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self._texts = texts\n",
    "        self._labels = labels\n",
    "        assert len(self._texts) == len(self._labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self._texts[index], torch.tensor(self._labels[index], dtype=torch.float)\n",
    "    \n",
    "    \n",
    "def build_bert_collate_fn(tokenizer):\n",
    "    def collate_fn(batch):\n",
    "        texts, labels = zip(*batch)\n",
    "        texts = list(texts)\n",
    "        texts = tokenizer(texts, return_tensors='pt', padding=True, max_length=tokenizer.model_max_length, truncation=True)\n",
    "        labels = torch.tensor(labels, dtype=torch.float)\n",
    "        return texts, labels\n",
    "    \n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IMDBBERTDataset(texts=train_df['processed_text'], labels=train_df['label'])\n",
    "valid_dataset = IMDBBERTDataset(texts=valid_df['processed_text'], labels=valid_df['label'])\n",
    "test_dataset = IMDBBERTDataset(texts=test_df['processed_text'], labels=test_df['label'])\n",
    "collate_fn = build_bert_collate_fn(tokenizer=BertTokenizer.from_pretrained(\"bert-base-uncased\"))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTTrainer(Trainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        network,\n",
    "        optimizer,\n",
    "        device,\n",
    "        num_epochs,\n",
    "        train_dataloader,\n",
    "        valid_dataloader,\n",
    "    ):\n",
    "        super().__init__(num_epochs, train_dataloader, valid_dataloader)\n",
    "        self._network = network\n",
    "        self._optimizer = optimizer\n",
    "        self._device = device\n",
    "        \n",
    "    def _run_batch(self, batch):\n",
    "        self._optimizer.zero_grad()\n",
    "        preds = self._network(**batch[0].to(device=self._device)).logits.squeeze()\n",
    "        loss = F.binary_cross_entropy_with_logits(preds, batch[1].to(device=self._device))\n",
    "        loss.backward()\n",
    "        self._optimizer.step()\n",
    "        \n",
    "        return preds, loss.item()\n",
    "    \n",
    "    def _inference(self, batch):\n",
    "        with torch.no_grad():\n",
    "            preds = self._network(**batch[0].to(device=self._device)).logits.squeeze()\n",
    "            loss = F.binary_cross_entropy_with_logits(preds, batch[1].to(device=self._device))\n",
    "            preds = (preds >= 0).to(dtype=torch.float32)\n",
    "            \n",
    "        return preds.cpu(), loss.item()\n",
    "    \n",
    "    def _load_model(self, path=None):\n",
    "        path = \"bert_model.ckpt\" if path is None else path\n",
    "        self._network.load_state_dict(torch.load(path))\n",
    "    \n",
    "    def _save_model(self):\n",
    "        torch.save(self._network.state_dict(), \"bert_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid acc: 0.9308, Test acc: 0.9350\n",
    "device = \"cpu\"\n",
    "network = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=1).to(device=device)\n",
    "optimizer = optim.Adam(network.parameters(), lr=3e-5)\n",
    "trainer = BERTTrainer(network, optimizer, device, 3, train_dataloader, valid_dataloader)\n",
    "trainer.train()\n",
    "\n",
    "trainer._load_model()\n",
    "trainer.evaluate(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A prototype of promping GPT-3.5/GPT-4 to do sentiment analysis (NOTE: this isn't fully run and debugged yet, be aware of your usage!)\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"YOUR_API_KEY_HERE\"\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "positive = train_df[(train_df[\"label\"] == 1) & (train_df[\"text_length\"] > 100) & (train_df[\"text_length\"] <= 1000)]\n",
    "negative = train_df[(train_df[\"label\"] == 0) & (train_df[\"text_length\"] > 100) & (train_df[\"text_length\"] <= 1000)]\n",
    "\n",
    "positive_samples = \"\\n\".join(positive.sample(2)[\"text\"].values)\n",
    "negative_samples = \"\\n\".join(negative.sample(2)[\"text\"].values)\n",
    "\n",
    "template = \"\"\"You're going to do a binary sentiment analysis task on movie reviews, where positive means that the review is positive regarding the movie, and vice versa. Here are some positive and negative samples.\n",
    "\n",
    "Positive ones are listed as below:\n",
    "{positive_samples}\n",
    "\n",
    "Negative ones are listed as below:\n",
    "{negative_samples}\n",
    "\n",
    "Now, tell me the sentiment of the review below. Only give me the word \\\"positive\\\" or \\\"negative\\\". Here is the review:\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def evaluate(df):\n",
    "    preds = []\n",
    "    for text in tqdm.tqdm(df[\"text\"]):\n",
    "        message = template.format(positive_samples=positive_samples, negative_samples=negative_samples, text=text)\n",
    "        pred = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": message},\n",
    "            ]\n",
    "        )[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "        preds.append(1 if \"positive\" in pred else 0)\n",
    "\n",
    "    preds = np.array(preds)\n",
    "    print(f\"Accuracy: {np.mean(preds == df['label'].values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
